<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"thomas-li-sjtu.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="基于简单粗暴TensorFlow2的笔记记录">
<meta property="og:type" content="article">
<meta property="og:title" content="简单粗暴TensorFlow2">
<meta property="og:url" content="https://thomas-li-sjtu.github.io/2020/11/07/%E7%AE%80%E5%8D%95%E7%B2%97%E6%9A%B4TensorFlow2/index.html">
<meta property="og:site_name" content="More Than Code">
<meta property="og:description" content="基于简单粗暴TensorFlow2的笔记记录">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-11-07T05:48:00.000Z">
<meta property="article:modified_time" content="2020-11-08T06:56:07.058Z">
<meta property="article:author" content="Thomas-Li">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="TensorFlow">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://thomas-li-sjtu.github.io/2020/11/07/%E7%AE%80%E5%8D%95%E7%B2%97%E6%9A%B4TensorFlow2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>简单粗暴TensorFlow2 | More Than Code</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/thomas-li-sjtu" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">More Than Code</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://thomas-li-sjtu.github.io/2020/11/07/%E7%AE%80%E5%8D%95%E7%B2%97%E6%9A%B4TensorFlow2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Thomas-Li">
      <meta itemprop="description" content="Stay hungry. Stay foolish.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="More Than Code">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          简单粗暴TensorFlow2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-07 13:48:00" itemprop="dateCreated datePublished" datetime="2020-11-07T13:48:00+08:00">2020-11-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>18k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>16 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>基于<a target="_blank" rel="noopener" href="https://tf.wiki/">简单粗暴TensorFlow2</a>的笔记记录</p>
<a id="more"></a>

<h1 id="TensorFlow-基础"><a href="#TensorFlow-基础" class="headerlink" title="TensorFlow 基础"></a>TensorFlow 基础</h1><h2 id="with语句"><a href="#with语句" class="headerlink" title="with语句"></a>with语句</h2><ul>
<li>with 语句包裹起来的代码块，在执行语句体之前会调用上下文管理器的  <code>__enter__()</code> ，执行完语句体之后会执行 <code>__exit__()</code> </li>
<li>上下文表达式（Context Expression）：with 语句中跟在关键字 with 之后的表达式，该表达式要返回一个上下文管理器对象</li>
<li>如果用as指定了一个目标，会将<code>__enter__()</code>返回值赋予这个目标<h3 id="其他介绍"><a href="#其他介绍" class="headerlink" title="其他介绍"></a>其他介绍</h3></li>
<li>可以用with 语句来代替 try…except…finally</li>
<li>一个上下文管理器（context manager）是一个对象，定义了运行一个with语句时候要建立的运行时上下文（runtime context）,掌控了何处进入，何处退出以及一个代码块运行所需的运行时上下文<h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2></li>
<li>张量 （Tensor）为TensorFlow中数据的基本单位，在概念上等同于多维数组——描述标量（0 维数组）、向量（1 维数组）、矩阵（2 维数组）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个随机数（标量）</span></span><br><span class="line">random_float = tf.random.uniform(shape=())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个有2个元素的零向量</span></span><br><span class="line">zero_vector = tf.zeros(shape=(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个2×2的常量矩阵</span></span><br><span class="line">A = tf.constant([[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">3.</span>, <span class="number">4.</span>]])</span><br></pre></td></tr></table></figure></li>
<li>属性包括形状、类型和值，通过张量的 <code>shape</code> 、 <code>dtype</code> 属性和 <code>numpy()</code> 方法获取</li>
<li>张量中元素的类型默认为 <code>tf.float32</code>，可以通过加入 <code>dtype</code> 参数来自行指定类型：<code>zero_vector = tf.zeros(shape=(2), dtype=tf.int32)</code></li>
<li>将已有的张量运算后得到新的张量：张量相加 <code>C = tf.add(A, B)</code><h2 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h2></li>
<li><code>tf.GradientTape()</code> 实现自动求导  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算y = x^2 在3的导数</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.Variable(initial_value=<span class="number">3.</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:     <span class="comment"># 在 tf.GradientTape() 的上下文内，所有计算步骤都会被记录以用于求导</span></span><br><span class="line">    y = tf.square(x)</span><br><span class="line">y_grad = tape.gradient(y, x)        <span class="comment"># 计算y关于x的导数</span></span><br><span class="line">print([y, y_grad])</span><br><span class="line"></span><br><span class="line">输出:</span><br><span class="line">[array([<span class="number">9.</span>], dtype=float32), array([<span class="number">6.</span>], dtype=float32)]</span><br></pre></td></tr></table></figure></li>
<li>x 是变量，同样具有形状、类型和值三种属性。变量需要有一个初始化过程，通过在 <code>tf.Variable()</code> 中指定 <code>initial_value</code> 初始化。</li>
<li>变量与普通张量的一个重要区别是其默认能够被自动求导机制所求导，因此常被用于定义模型的参数</li>
<li>同样可计算偏导、矩阵或向量的导数</li>
<li><code>tf.square()</code> ：对输入张量的每一个元素求平方，不改变张量形状</li>
<li><code>tf.reduce_sum()</code> ：对输入张量的所有元素求和，输出一个形状为空的纯量张量，可通过 <code>axis</code> 参数来指定求和的维度<h2 id="示例：线性回归"><a href="#示例：线性回归" class="headerlink" title="示例：线性回归"></a>示例：线性回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对一组离散值（X,Y）线性回归</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 定义数据</span></span><br><span class="line">X_raw = np.array([<span class="number">2013</span>, <span class="number">2014</span>, <span class="number">2015</span>, <span class="number">2016</span>, <span class="number">2017</span>], dtype=np.float32)</span><br><span class="line">y_raw = np.array([<span class="number">12000</span>, <span class="number">14000</span>, <span class="number">15000</span>, <span class="number">16500</span>, <span class="number">17500</span>], dtype=np.float32)</span><br><span class="line"><span class="comment"># 数据归一化</span></span><br><span class="line">X = (X_raw - X_raw.<span class="built_in">min</span>()) / (X_raw.<span class="built_in">max</span>() - X_raw.<span class="built_in">min</span>())</span><br><span class="line">y = (y_raw - y_raw.<span class="built_in">min</span>()) / (y_raw.<span class="built_in">max</span>() - y_raw.<span class="built_in">min</span>())</span><br><span class="line"></span><br><span class="line">* 梯度下降过程：</span><br><span class="line">  * 初始化自变量为$x_0$，$k=<span class="number">0</span>$</span><br><span class="line">  * 迭代：</span><br><span class="line">    * 求函数$f(x)$关于自变量$x$的梯度$\nabla f(x_k)$</span><br><span class="line">    * 更新自变量：$x_&#123;k+<span class="number">1</span>&#125;=x_k-\gamma \nabla f(x_k)$，$\gamma$为学习率</span><br><span class="line">    * $k\leftarrow k+<span class="number">1</span>$</span><br><span class="line">    ```python</span><br><span class="line">    X = tf.constant(X)</span><br><span class="line">    y = tf.constant(y)</span><br><span class="line"></span><br><span class="line">    a = tf.Variable(initial_value=<span class="number">0.</span>)</span><br><span class="line">    b = tf.Variable(initial_value=<span class="number">0.</span>)</span><br><span class="line">    variables = [a, b]</span><br><span class="line"></span><br><span class="line">    num_epoch = <span class="number">10000</span></span><br><span class="line">    optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">1e-3</span>) </span><br><span class="line">    <span class="comment">#声明了一个梯度下降优化器 Optimizer，学习率为 1e-3，计算出求导结果更新模型参数，以最小化特定的损失函数，具体使用方式是调用其apply_gradients() 方法</span></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">        <span class="comment"># 使用tf.GradientTape()记录损失函数的梯度信息</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y_pred = a * X + b</span><br><span class="line">            loss = <span class="number">0.5</span> * tf.reduce_sum(tf.square(y_pred - y))</span><br><span class="line">        <span class="comment"># TensorFlow自动计算损失函数关于自变量（模型参数）的梯度——求出 tape 中记录的 loss 关于 variables = [a, b] 中每个变量的偏导数</span></span><br><span class="line">        grads = tape.gradient(loss, variables)</span><br><span class="line">        <span class="comment"># TensorFlow自动根据梯度更新参数</span></span><br><span class="line">        <span class="comment"># 参数 grads_and_vars，即待更新的变量，和损失函数关于这些变量的偏导数</span></span><br><span class="line">        optimizer.apply_gradients(grads_and_vars=<span class="built_in">zip</span>(grads, variables))</span><br><span class="line"></span><br><span class="line">    print(a, b)</span><br></pre></td></tr></table></figure></li>
<li><code>zip()</code> 为内置函数：若 a = [1, 3, 5]， b = [2, 4, 6]，则 <code>zip(a, b) = [(1, 2), (3, 4),  (5, 6)]</code>。<code>zip()</code> 返回一个 zip 对象，本需要调用 <code>list()</code> 来将生成器转换成列表</li>
</ul>
<h1 id="TensorFlow-模型建立与训练"><a href="#TensorFlow-模型建立与训练" class="headerlink" title="TensorFlow 模型建立与训练"></a>TensorFlow 模型建立与训练</h1><h2 id="模型（Model）与层（Layer）"><a href="#模型（Model）与层（Layer）" class="headerlink" title="模型（Model）与层（Layer）"></a>模型（Model）与层（Layer）</h2><ul>
<li><p>常使用 Keras（ <code>tf.keras</code> ）构建模型</p>
</li>
<li><p>有两个重要概念： 模型（Model） 和 层（Layer）</p>
<ul>
<li>层封装各种计算流程和变量</li>
<li>模型组织和连接各种层，封装成一个整体</li>
</ul>
</li>
<li><p>Keras 模型以类的形式使用，可以通过继承 tf.keras.Model 类来定义新的模型。继承类中，需要重写<code>__init__()</code>（构造函数，初始化）和  <code>call(input)</code>（模型调用）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="built_in">super</span>().__init__()     </span><br><span class="line">      <span class="comment"># 添加初始化代码（包含 call 方法中会用到的层），例如</span></span><br><span class="line">      <span class="comment"># layer1 = tf.keras.layers.BuiltInLayer(...)</span></span><br><span class="line">      <span class="comment"># layer2 = MyCustomLayer(...)</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">      <span class="comment"># 添加模型调用的代码（处理输入并返回输出），例如</span></span><br><span class="line">      <span class="comment"># x = layer1(input)</span></span><br><span class="line">      <span class="comment"># output = layer2(x)</span></span><br><span class="line">      <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="Keras-的全连接层"><a href="#Keras-的全连接层" class="headerlink" title="Keras 的全连接层"></a>Keras 的全连接层</h3></li>
<li><p><code>tf.keras.layers.Dense</code>输入矩阵 $A$ 进行 $f(AW + b)$ 的线性变换 + 激活函数操作。不指定激活函数，则为线性变换 $AW + b$。</p>
</li>
<li><p>输入矩阵 $A$ <code>= [batch_size, input_dim]</code></p>
</li>
<li><p>输出二维张量 <code>[batch_size, units]</code> </p>
</li>
<li><p>常用的激活函数包括 <code>tf.nn.relu</code>、<code>tf.nn.tanh</code>和<code>tf.nn.sigmoid </code></p>
</li>
<li><p>可训练变量：</p>
<ul>
<li>权重矩阵 $W$:<code>kernel = [input_dim, units]</code> </li>
<li>偏置向量 $b$: <code>bias = [units]</code> <h2 id="示例：多层感知机-MLP"><a href="#示例：多层感知机-MLP" class="headerlink" title="示例：多层感知机 MLP"></a>示例：多层感知机 MLP</h2></li>
</ul>
</li>
<li><p>多层全连接神经网络</p>
</li>
<li><p>完成 MNIST 手写体数字图片数据集的分类任务</p>
<h3 id="数据获取及预处理：-tf-keras-datasets"><a href="#数据获取及预处理：-tf-keras-datasets" class="headerlink" title="数据获取及预处理： tf.keras.datasets"></a>数据获取及预处理： tf.keras.datasets</h3></li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNISTLoader</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        mnist = tf.keras.datasets.mnist <span class="comment"># 从网上自动下载 MNIST 数据集并加载，放置在：C:\Users\ASUS\.keras/dataset 目录下</span></span><br><span class="line">        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()  <span class="comment"># 载入数据</span></span><br><span class="line">        <span class="comment"># MNIST中的图像默认为uint8（0-255的数字）。将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道</span></span><br><span class="line">        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / <span class="number">255.0</span>, axis=<span class="number">-1</span>)      <span class="comment"># [60000, 28, 28, 1]</span></span><br><span class="line">        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / <span class="number">255.0</span>, axis=<span class="number">-1</span>)        <span class="comment"># [10000, 28, 28, 1]</span></span><br><span class="line">        self.train_label = self.train_label.astype(np.int32)    <span class="comment"># [60000]</span></span><br><span class="line">        self.test_label = self.test_label.astype(np.int32)      <span class="comment"># [10000]</span></span><br><span class="line">        self.num_train_data, self.num_test_data = self.train_data.shape[<span class="number">0</span>], self.test_data.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_batch</span>(<span class="params">self, batch_size</span>):</span></span><br><span class="line">        <span class="comment"># 从数据集中随机取出batch_size个元素并返回</span></span><br><span class="line">        index = np.random.randint(<span class="number">0</span>, np.shape(self.train_data)[<span class="number">0</span>], batch_size)</span><br><span class="line">        <span class="keyword">return</span> self.train_data[index, :], self.train_label[index]</span><br></pre></td></tr></table></figure></li>
<li><p>图像数据集的一种典型表示是 <code>[图像数目，长，宽，色彩通道数]</code> 的四维张量</p>
<h3 id="模型的构建：-tf-keras-Model-和-tf-keras-layers"><a href="#模型的构建：-tf-keras-Model-和-tf-keras-layers" class="headerlink" title="模型的构建： tf.keras.Model 和 tf.keras.layers"></a>模型的构建： tf.keras.Model 和 tf.keras.layers</h3></li>
<li><p>引入非线性激活函数 <code>ReLU</code></p>
</li>
<li><p>输入一个向量（这里是拉直的 1×784 手写体数字图片），输出 10 维的向量，代表图片属于 0 到 9 的概率</p>
</li>
<li><p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="built_in">super</span>().__init__()</span><br><span class="line">      self.flatten = tf.keras.layers.Flatten()    <span class="comment"># Flatten层将除第一维（batch_size）以外的维度展平   这也对应神经网络的输入层</span></span><br><span class="line">      self.dense1 = tf.keras.layers.Dense(units=<span class="number">100</span>, activation=tf.nn.relu) <span class="comment"># 中间层</span></span><br><span class="line">      self.dense2 = tf.keras.layers.Dense(units=<span class="number">10</span>) <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span>         <span class="comment"># [batch_size, 28, 28, 1]</span></span><br><span class="line">      x = self.flatten(inputs)    <span class="comment"># [batch_size, 784]</span></span><br><span class="line">      x = self.dense1(x)          <span class="comment"># [batch_size, 100]</span></span><br><span class="line">      x = self.dense2(x)          <span class="comment"># [batch_size, 10]</span></span><br><span class="line">      output = tf.nn.softmax(x)</span><br><span class="line">      <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></li>
<li><p><code>tf.nn.softmax</code>: 归一化指数函数，将模型的原始输出归一化，使输出向量：</p>
<ul>
<li>该向量的每个元素属于 [0, 1] </li>
<li>该向量所有元素之和为 1</li>
</ul>
</li>
<li><p>原本输入为 28*28 的图片（向量），通过 <code>tf.keras.layers.Flatten()</code> 将其展平为 784*1 的向量</p>
<h3 id="2-2-3-模型的训练：-tf-keras-losses-和-tf-keras-optimizer"><a href="#2-2-3-模型的训练：-tf-keras-losses-和-tf-keras-optimizer" class="headerlink" title="2.2.3. 模型的训练： tf.keras.losses 和 tf.keras.optimizer"></a>2.2.3. 模型的训练： tf.keras.losses 和 tf.keras.optimizer</h3></li>
<li><p><code>tf.keras.losses</code> 计算损失函数，<code>tf.keras.optimizer</code> 优化模型</p>
</li>
<li><p>定义模型超参数——模型超参数是模型外部的配置，其值不能从数据估计</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br></pre></td></tr></table></figure></li>
<li><p>实例化模型和数据读取类，并实例一个优化器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = MLP()</span><br><span class="line">data_loader = MNISTLoader()</span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) <span class="comment"># Adam优化器</span></span><br></pre></td></tr></table></figure></li>
<li><p>迭代：</p>
<ul>
<li>DataLoader 中随机取一批训练数据送入模型，计算出模型的预测值</li>
<li>模型预测值与真实值进行比较，计算损失函数（loss）——使用交叉熵</li>
<li>计算损失函数关于模型变量的导数</li>
<li>导数值传入优化器，使用 <code>apply_gradients</code> 更新模型参数以最小化损失函数</li>
</ul>
</li>
<li><p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_batches = <span class="built_in">int</span>(data_loader.num_train_data // batch_size * num_epochs)</span><br><span class="line"><span class="keyword">for</span> batch_index <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">    X, y = data_loader.get_batch(batch_size)</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y_pred = model(X)</span><br><span class="line">        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)  <span class="comment"># 交叉熵函数，参数为模型预测值和真实标签值</span></span><br><span class="line">        loss = tf.reduce_mean(loss)</span><br><span class="line">        print(<span class="string">&quot;batch %d: loss %f&quot;</span> % (batch_index, loss.numpy()))</span><br><span class="line">    grads = tape.gradient(loss, model.variables)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars=<span class="built_in">zip</span>(grads, model.variables))</span><br></pre></td></tr></table></figure>
<h3 id="模型的评估：-tf-keras-metrics"><a href="#模型的评估：-tf-keras-metrics" class="headerlink" title="模型的评估： tf.keras.metrics"></a>模型的评估： tf.keras.metrics</h3></li>
<li><p>测试集评估模型的性能：<code>tf.keras.metrics</code> 中的 <code>SparseCategoricalAccuracy</code> 评估器来评估性能，能够对模型预测的结果与真实结果进行比较，并输出预测正确的样本数占总样本数的比例</p>
</li>
<li><p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()  <span class="comment"># 实例化一个评估器</span></span><br><span class="line">num_batches = <span class="built_in">int</span>(data_loader.num_test_data // batch_size)</span><br><span class="line"><span class="keyword">for</span> batch_index <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">    start_index, end_index = batch_index * batch_size, (batch_index + <span class="number">1</span>) * batch_size </span><br><span class="line">    y_pred = model.predict(data_loader.test_data[start_index: end_index]) </span><br><span class="line">    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred) <span class="comment"># 参数为预测出的结果和真实结果</span></span><br><span class="line">print(<span class="string">&quot;test accuracy: %f&quot;</span> % sparse_categorical_accuracy.result()) <span class="comment"># result() 输出评估指标值</span></span><br></pre></td></tr></table></figure>
<h2 id="卷积神经网络-CNN"><a href="#卷积神经网络-CNN" class="headerlink" title="卷积神经网络 CNN"></a>卷积神经网络 CNN</h2><h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3></li>
<li><p>结构类似于视觉系统的人工神经网络，包含一个或多个卷积层（Convolutional Layer）、池化层（Pooling Layer）和全连接层</p>
<blockquote>
<p>CNN中的卷积：<br> $$ s(t) = (X<em>W)(t) $$<br> $$ s(i,j) = (X</em>W)(i,j) = \sum_m \sum_n x(i+m,j+n)w(m,n) $$<br>直观上，两个矩阵对应位置元素相乘，之后相加<br>$W$为卷积核，也称感受野，$X$为输入<br>有时会存在偏置</p>
</blockquote>
</li>
<li><p>卷积层+池化层（卷积层+卷积层）的组合可以在隐藏层出现很多次，最常见的 CNN 都是若干卷积层+池化层的组合</p>
</li>
<li><p>输出层使用 <code>Softmax</code> 激活函数来做图像识别的分类</p>
</li>
<li><p>卷积层：</p>
<ul>
<li>进行特征提取</li>
<li>感受野深度和图像深度应当一致</li>
<li>输入 3*4 ，核矩阵 2*2，则输出为 2*3</li>
<li>对于卷积后的输出，一般会通过 <code>ReLU</code> 激活函数，将输出的张量中小于0的位置对应的元素值都变为0</li>
<li>示例：<br>![](简单粗暴 TensorFlow 2/1.PNG)<ul>
<li>这里图片原本为 5*5，但增加了一个边界，边界元素为 0（+pad 1）。增加的圈数一般为经验值</li>
<li>此处卷积步幅为2，感受野深度和图像深度为3——卷积核由三个矩阵组成</li>
<li>两个卷积核的对应偏置不同</li>
</ul>
</li>
</ul>
</li>
<li><p>池化层：</p>
<ul>
<li>对输入张量的各个子矩阵进行压缩，使特征图变小，并将特征压缩</li>
<li>常见的池化标准有2个，<code>MAX Pooling</code>或者是<code>Average Pooling</code>。即取对应区域的最大值或者平均值作为池化后的元素值——将 n*n 的矩阵变成一个元素<h3 id="2-3-2-Keras-实现卷积神经网络"><a href="#2-3-2-Keras-实现卷积神经网络" class="headerlink" title="2.3.2. Keras 实现卷积神经网络"></a>2.3.2. Keras 实现卷积神经网络</h3></li>
</ul>
</li>
<li><p>CNN 类与多层感知机 类相似</p>
</li>
<li><p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="built_in">super</span>().__init__()</span><br><span class="line">      self.conv1 = tf.keras.layers.Conv2D(</span><br><span class="line">          filters=<span class="number">32</span>,             <span class="comment"># 卷积层神经元（卷积核）数目</span></span><br><span class="line">          kernel_size=[<span class="number">5</span>, <span class="number">5</span>],     <span class="comment"># 感受野大小</span></span><br><span class="line">          padding=<span class="string">&#x27;same&#x27;</span>,         <span class="comment"># padding策略（vaild 或 same）</span></span><br><span class="line">          activation=tf.nn.relu   <span class="comment"># 激活函数</span></span><br><span class="line">      )</span><br><span class="line">      self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>)  <span class="comment"># 池化</span></span><br><span class="line">      self.conv2 = tf.keras.layers.Conv2D(</span><br><span class="line">          filters=<span class="number">64</span>,</span><br><span class="line">          kernel_size=[<span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">          padding=<span class="string">&#x27;same&#x27;</span>,</span><br><span class="line">          activation=tf.nn.relu</span><br><span class="line">      )</span><br><span class="line">      self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>) <span class="comment"># 池化</span></span><br><span class="line">      self.flatten = tf.keras.layers.Reshape(target_shape=(<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>,))</span><br><span class="line">      self.dense1 = tf.keras.layers.Dense(units=<span class="number">1024</span>, activation=tf.nn.relu)</span><br><span class="line">      self.dense2 = tf.keras.layers.Dense(units=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">      x = self.conv1(inputs)                  <span class="comment"># [batch_size, 28, 28, 32]</span></span><br><span class="line">      x = self.pool1(x)                       <span class="comment"># [batch_size, 14, 14, 32]</span></span><br><span class="line">      x = self.conv2(x)                       <span class="comment"># [batch_size, 14, 14, 64]</span></span><br><span class="line">      x = self.pool2(x)                       <span class="comment"># [batch_size, 7, 7, 64]</span></span><br><span class="line">      x = self.flatten(x)                     <span class="comment"># [batch_size, 7 * 7 * 64]</span></span><br><span class="line">      x = self.dense1(x)                      <span class="comment"># [batch_size, 1024]</span></span><br><span class="line">      x = self.dense2(x)                      <span class="comment"># [batch_size, 10]</span></span><br><span class="line">      output = tf.nn.softmax(x)</span><br><span class="line">      <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="2-3-3-Keras-中预定义的卷积神经网络"><a href="#2-3-3-Keras-中预定义的卷积神经网络" class="headerlink" title="2.3.3. Keras 中预定义的卷积神经网络"></a>2.3.3. Keras 中预定义的卷积神经网络</h3></li>
<li><p><code>tf.keras.applications</code> 中有预定义好的经典卷积神经网络结构，如 <code>VGG16</code> 、 <code>VGG19</code> 、 <code>ResNet</code> 、 <code>MobileNet</code> 等</p>
</li>
<li><p>实例化 <code>MobileNet</code>（参数均取默认值）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.applications.MobileNetV2()</span><br></pre></td></tr></table></figure></li>
<li><p>一些通用的模型参数：</p>
<ul>
<li><code>input_shape</code> ：输入张量的形状（不含第一维的 Batch）</li>
<li><code>include_top</code> ：网络是否包含全连接层，默认为 <code>True</code></li>
<li><code>weights</code> ：预训练权值，默认为 <code>&#39;imagenet&#39;</code>，即模型载入在 ImageNet 数据集上预训练的权值。如需随机初始化变量，设为 <code>None</code></li>
<li><code>classes</code> ：分类数（类似于输出层的神经元数目），默认为 1000。修改时需要 <code>include_top</code> 参数为 <code>True</code> 且 <code>weights</code> 参数为 <code>None</code><h2 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络 RNN"></a>循环神经网络 RNN</h2><h3 id="预备知识-1"><a href="#预备知识-1" class="headerlink" title="预备知识"></a>预备知识</h3></li>
</ul>
</li>
<li><p>适合处理序列数据——语言模型、文本生成、机器翻译——对于一个序列，当前的输出与前面的输出也有关</p>
</li>
<li><p>隐藏层之间的节点有连接，隐藏层的输入来自输入层的输出和上一时刻隐藏层的输出<br>![](简单粗暴 TensorFlow 2/2.PNG)</p>
<blockquote>
<p>$$s_t = f(Us_t + Wx_{t-1}) $$</p>
</blockquote>
</li>
<li><p>RNN 对于长时记忆的困难主要来自梯度爆炸/梯度消失</p>
<h3 id="2-4-2-示例：RNN-自动生成尼采风格文本"><a href="#2-4-2-示例：RNN-自动生成尼采风格文本" class="headerlink" title="2.4.2. 示例：RNN 自动生成尼采风格文本"></a>2.4.2. 示例：RNN 自动生成尼采风格文本</h3></li>
<li><p>设置 <code>DataLoader</code> 类读取文本，以字符为单位进行编码：字符种类数为 <code>num_chars</code>，每种字符赋予一个唯一整数编号 <code>i</code>（0 到 num_chars-1）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>():</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span> <span class="comment"># 创建对象会自动调用初始化方法__init__</span></span><br><span class="line">      path = tf.keras.utils.get_file(<span class="string">&#x27;nietzsche.txt&#x27;</span>,</span><br><span class="line">          origin=<span class="string">&#x27;https://s3.amazonaws.com/text-datasets/nietzsche.txt&#x27;</span>)  <span class="comment"># 从远程网址下载数据，并保存到文件中</span></span><br><span class="line">                                                                          <span class="comment"># tf.keras.utils.get_file(fname=TRAIN_URL.split(&#x27;/&#x27;)[-1],orgin=TRAIN_URL, cache_dir=&#x27;.&#x27;) fname:文件命名；origin:文件的url地址；cache_dir:存储地址，此处指存储到当前目录下，默认为：C:\Users\ASUS\.keras\datasets</span></span><br><span class="line">      <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">          self.raw_text = f.read().lower()</span><br><span class="line">      self.chars = <span class="built_in">sorted</span>(<span class="built_in">list</span>(<span class="built_in">set</span>(self.raw_text))) <span class="comment"># set()创建一个无序不重复元素集</span></span><br><span class="line">                                                    <span class="comment"># list()将元组转换为列表</span></span><br><span class="line">                                                    <span class="comment"># sorted()对所有可迭代的对象从小到大排序</span></span><br><span class="line">      self.char_indices = <span class="built_in">dict</span>((c, i) <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.chars)) <span class="comment"># enumerate()将可遍历数据对象组合为一个索引序列，序列的元素为数据和数据对应下标，即(0,数据0),(1,数据1)</span></span><br><span class="line">                                                                         <span class="comment"># dict() 转为字典，如&#123;&#x27;a&#x27;: 0, &#x27;h&#x27;: 1&#125;</span></span><br><span class="line">      self.indices_char = <span class="built_in">dict</span>((i, c) <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.chars))</span><br><span class="line">      self.text = [self.char_indices[c] <span class="keyword">for</span> c <span class="keyword">in</span> self.raw_text] <span class="comment"># 根据字典的索引找到对应数字，如&#x27;ah&#x27;变为[1,0]</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_batch</span>(<span class="params">self, seq_length, batch_size</span>):</span>  </span><br><span class="line">      seq = []</span><br><span class="line">      next_char = []</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">          index = np.random.randint(<span class="number">0</span>, <span class="built_in">len</span>(self.text) - seq_length)</span><br><span class="line">          seq.append(self.text[index:index+seq_length])</span><br><span class="line">          next_char.append(self.text[index+seq_length])</span><br><span class="line">      <span class="keyword">return</span> np.array(seq), np.array(next_char)       <span class="comment"># [batch_size, seq_length], [num_batch]  np即numpy</span></span><br></pre></td></tr></table></figure></li>
<li><p>模型的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_chars, batch_size, seq_length</span>):</span></span><br><span class="line">      <span class="built_in">super</span>().__init__()</span><br><span class="line">      self.num_chars = num_chars </span><br><span class="line">      self.seq_length = seq_length</span><br><span class="line">      self.batch_size = batch_size</span><br><span class="line">      self.cell = tf.keras.layers.LSTMCell(units=<span class="number">256</span>) <span class="comment"># 实例化LSTMCell</span></span><br><span class="line">      self.dense = tf.keras.layers.Dense(units=self.num_chars) <span class="comment"># 实例化全连接层</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, from_logits=<span class="literal">False</span></span>):</span></span><br><span class="line">      inputs = tf.one_hot(inputs, depth=self.num_chars)       <span class="comment"># [batch_size, seq_length, num_chars]  </span></span><br><span class="line">                                                              <span class="comment"># 编码：独热码，序列转为[seq_length,num_chars] 张量</span></span><br><span class="line">      state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32) <span class="comment"># 初始化cell状态</span></span><br><span class="line">      <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(self.seq_length): <span class="comment"># 将序列送入RNN单元</span></span><br><span class="line">          output, state = self.cell(inputs[:, t, :], state) <span class="comment"># 当前</span></span><br><span class="line">      logits = self.dense(output)  <span class="comment"># 取最后一次的输出</span></span><br><span class="line">      <span class="keyword">if</span> from_logits:</span><br><span class="line">          <span class="keyword">return</span> logits</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">return</span> tf.nn.softmax(logits)</span><br></pre></td></tr></table></figure></li>
<li><p>训练：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line">num_batches = <span class="number">1000</span></span><br><span class="line">seq_length = <span class="number">40</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">data_loader = DataLoader()</span><br><span class="line">model = RNN(num_chars=<span class="built_in">len</span>(data_loader.chars), batch_size=batch_size, seq_length=seq_length)</span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)</span><br><span class="line"><span class="keyword">for</span> batch_index <span class="keyword">in</span> <span class="built_in">range</span>(num_batches): <span class="comment"># 迭代</span></span><br><span class="line">    X, y = data_loader.get_batch(seq_length, batch_size)  <span class="comment"># 取数据</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment"># 送入模型，计算损失函数</span></span><br><span class="line">        y_pred = model(X)</span><br><span class="line">        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)</span><br><span class="line">        loss = tf.reduce_mean(loss)</span><br><span class="line">        print(<span class="string">&quot;batch %d: loss %f&quot;</span> % (batch_index, loss.numpy()))</span><br><span class="line">    grads = tape.gradient(loss, model.variables) <span class="comment"># 关于求导</span></span><br><span class="line">    optimizer.apply_gradients(grads_and_vars=<span class="built_in">zip</span>(grads, model.variables)) <span class="comment"># 优化器更新模型</span></span><br></pre></td></tr></table></figure></li>
<li><p>预测与生成文本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, inputs, temperature=<span class="number">1.</span></span>):</span> <span class="comment"># temperature参数控制分布的形状，参数越大分布越平缓（最大值和最小值差值小）</span></span><br><span class="line">      batch_size, _ = tf.shape(inputs)</span><br><span class="line">      logits = self(inputs, from_logits=<span class="literal">True</span>)</span><br><span class="line">      prob = tf.nn.softmax(logits / temperature).numpy()</span><br><span class="line">      <span class="keyword">return</span> np.array([np.random.choice(self.num_chars, p=prob[i, :])</span><br><span class="line">                       <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size.numpy())])</span><br><span class="line"></span><br><span class="line">X_, _ = data_loader.get_batch(seq_length, <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">for</span> diversity <span class="keyword">in</span> [<span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">1.2</span>]:</span><br><span class="line">      X = X_</span><br><span class="line">      print(<span class="string">&quot;diversity %f:&quot;</span> % diversity)</span><br><span class="line">      <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">400</span>):</span><br><span class="line">          y_pred = model.predict(X, diversity)</span><br><span class="line">          print(data_loader.indices_char[y_pred[<span class="number">0</span>]], end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">          X = np.concatenate([X[:, <span class="number">1</span>:], np.expand_dims(y_pred, axis=<span class="number">1</span>)], axis=<span class="number">-1</span>)</span><br><span class="line">      print(<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="TensorFlow-常用模块"><a href="#TensorFlow-常用模块" class="headerlink" title="TensorFlow 常用模块"></a>TensorFlow 常用模块</h1><h2 id="预备知识：Python迭代器"><a href="#预备知识：Python迭代器" class="headerlink" title="预备知识：Python迭代器"></a>预备知识：Python迭代器</h2></li>
<li><p>迭代器是一个可以记住遍历位置的对象，有两个基本的方法：<code>iter()</code> 和 <code>next()</code></p>
</li>
<li><p>每调用一次 <code>next()</code> 返回下一个元素，若下一个元素不存在返回 <code>StopIteration </code>异常</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">&#x27;ab&#x27;</span></span><br><span class="line">it = <span class="built_in">iter</span>(s)</span><br><span class="line">print(it.__next__())</span><br><span class="line">   <span class="string">&#x27;a&#x27;</span></span><br><span class="line">print(it.__next__())</span><br><span class="line">   <span class="string">&#x27;b&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="tf-train-Checkpoint类-：变量的保存与恢复"><a href="#tf-train-Checkpoint类-：变量的保存与恢复" class="headerlink" title="tf.train.Checkpoint类 ：变量的保存与恢复"></a>tf.train.Checkpoint类 ：变量的保存与恢复</h2></li>
<li><p>方法 <code>save()</code> 和 <code>restore()</code> 保存和恢复 <code>Checkpointable State</code> 的对象——<code>tf.keras.optimizer</code> 、 <code>tf.Variable</code> 、 <code>tf.keras.Layer</code> 或者 <code>tf.keras.Model</code> 实例</p>
<h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3></li>
<li><p>只保存模型的参数，不保存模型的计算过程</p>
</li>
<li><p><code>tf.train.Checkpoint()</code> 的实例，接受的初始化参数是一个 <code>**kwargs</code>，即一系列的键值对。键名随意取，值为需要保存的对象。恢复变量的时候，还将使用同一键名</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 举例：</span></span><br><span class="line">checkpointA = tf.train.Checkpoint(model=model) <span class="comment"># 实例化</span></span><br><span class="line">checkpointB = tf.train.Checkpoint(myAwesomeModel=model, myAwesomeOptimizer=optimizer)</span><br></pre></td></tr></table></figure></li>
<li><p>模型训练后需要保存时：<code>checkpointA.save(&#39;保存文件目录+前缀&#39;)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 具体框架如下：</span></span><br><span class="line">model = MyModel()</span><br><span class="line"><span class="comment"># 实例化Checkpoint，指定保存对象为model</span></span><br><span class="line">checkpoint = tf.train.Checkpoint(myModel=model)</span><br><span class="line"><span class="comment"># ...（模型训练代码）</span></span><br><span class="line"><span class="comment"># 模型训练完毕后将参数保存到文件（也可在模型训练过程中每隔一段时间就保存一次）</span></span><br><span class="line">checkpoint.save(<span class="string">&#x27;./save/model.ckpt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-2-2-载入"><a href="#3-2-2-载入" class="headerlink" title="3.2.2. 载入"></a>3.2.2. 载入</h3></li>
<li><p>为模型重新载入之前保存的参数时，再次实例化一个 <code>checkpoint</code>，并保持键名一致</p>
</li>
<li><p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_to_be_restored = MyModel()                                        <span class="comment"># 待恢复参数的同一模型</span></span><br><span class="line">checkpoint = tf.train.Checkpoint(myAwesomeModel=model_to_be_restored)   <span class="comment"># 键名保持为“myAwesomeModel”</span></span><br><span class="line">checkpoint.restore(save_path_with_prefix_and_index) <span class="comment"># 之前保存的文件的目录 + 前缀 + 编号，即：checkpoint.restore(&#x27;./save/model.ckpt-1&#x27;) ，载入前缀为 model.ckpt，序号为 1 的文件来恢复模型</span></span><br></pre></td></tr></table></figure></li>
<li><p>载入最新的模型： <code>tf.train.latest_checkpoint(save_path)</code></p>
<h3 id="删除旧的-Checkpoint"><a href="#删除旧的-Checkpoint" class="headerlink" title="删除旧的 Checkpoint"></a>删除旧的 Checkpoint</h3></li>
<li><p><code>tf.train.CheckpointManager</code></p>
<ul>
<li>训练过程产生大量 <code>Checkpoint</code>，但只想保留最后的几个</li>
<li>使用别的编号方式（例如使用当前 Batch 的编号作为文件编号</li>
</ul>
</li>
<li><p>在定义 <code>Checkpoint</code> 后定义一个 <code>CheckpointManager</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = tf.train.Checkpoint(model=model)</span><br><span class="line">manager = tf.train.CheckpointManager(checkpoint, directory=<span class="string">&#x27;./save&#x27;</span>, checkpoint_name=<span class="string">&#x27;model.ckpt&#x27;</span>, max_to_keep=k)</span><br></pre></td></tr></table></figure></li>
<li><p><code>directory</code> 为文件保存的路径， <code>checkpoint_name</code> 为文件名前缀（默认为ckpt）， <code>max_to_keep</code> 为保留的 <code>Checkpoint</code> 数目</p>
</li>
<li><p><code>manager.save()</code>：</p>
<ul>
<li>保存模型</li>
<li>自行指定保存的 <code>Checkpoint</code> 的编号——加入 <code>checkpoint_number</code> 参数，如 <code>manager.save(checkpoint_number=100)</code><h2 id="TensorBoard：训练过程可视化"><a href="#TensorBoard：训练过程可视化" class="headerlink" title="TensorBoard：训练过程可视化"></a>TensorBoard：训练过程可视化</h2></li>
</ul>
</li>
<li><p>代码目录下建立文件夹（如 <code>./tensorboard</code> ）存放记录文件，并实例化一个记录器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary_writer = tf.summary.create_file_writer(<span class="string">&#x27;./tensorboard&#x27;</span>)     <span class="comment"># 参数为保存的目录</span></span><br></pre></td></tr></table></figure></li>
<li><p><code>with</code> 语句指定使用的记录器，并记录参数在 <code>step</code> 时候的值： <code>tf.summary.scalar(name, tensor, step=batch_index)</code>（参数为标量时，用 <code>scalar()</code> ）。运行一次 <code>tf.summary.scalar()</code> ，记录器向记录文件中写入一条记录</p>
</li>
<li><p>一般设置 <code>step</code> 为当前训练过程中的 <code>batch</code> 序号。代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">summary_writer = tf.summary.create_file_writer(<span class="string">&#x27;./tensorboard&#x27;</span>)</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="keyword">for</span> batch_index <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">    <span class="comment"># ...（训练代码，变量loss存储batch的损失值）</span></span><br><span class="line">    <span class="keyword">with</span> summary_writer.as_default():        <span class="comment"># 希望使用的记录器</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&quot;loss&quot;</span>, loss, step=batch_index)</span><br><span class="line">        tf.summary.scalar(<span class="string">&quot;MyScalar&quot;</span>, my_scalar, step=batch_index)</span><br></pre></td></tr></table></figure></li>
<li><p>代码目录打开终端，运行 <code>tensorboard --logdir=./tensorboard</code>，访问命令行所输出的网址</p>
<h2 id="tf-data-：数据集的构建与预处理"><a href="#tf-data-：数据集的构建与预处理" class="headerlink" title="tf.data ：数据集的构建与预处理"></a>tf.data ：数据集的构建与预处理</h2><h3 id="数据集对象的建立"><a href="#数据集对象的建立" class="headerlink" title="数据集对象的建立"></a>数据集对象的建立</h3></li>
<li><p><code>tf.data</code> 的核心是 <code>tf.data.Dataset</code> 类，由一系列可迭代访问的元素组成——元素包含一个或多个张量（如：一个 长*宽*通道数 的图片张量或图片张量与标签张量组成的元组）</p>
</li>
<li><p><code>tf.data.Dataset.from_tensor_slices()</code> 适用于数据能整个装入内存的情况——所有张量可拼接成一个大的张量</p>
<ul>
<li>如[60000,28,28,1](60000张 28*28的单通道灰度图像)，此时数据集元素数量和第0维大小相同，即60000</li>
<li>多个张量作为输入时，第0维必须相同，且必须将张量拼接成元组<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">X = tf.constant([<span class="number">2013</span>, <span class="number">2014</span>, <span class="number">2015</span>, <span class="number">2016</span>, <span class="number">2017</span>])</span><br><span class="line">Y = tf.constant([<span class="number">12000</span>, <span class="number">14000</span>, <span class="number">15000</span>, <span class="number">16500</span>, <span class="number">17500</span>]) <span class="comment"># 创建常量张量</span></span><br><span class="line"></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((X, Y)) <span class="comment"># 形成[5,2]的张量</span></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> dataset:</span><br><span class="line">    print(x.numpy(), y.numpy()) </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>下载keras的datasets中得数据： <code>data = tf.keras.datasets.mnist.load_data()</code></p>
</li>
<li><p>对特别巨大而无法完整载入内存的数据集，可以先将数据集处理为 TFRecord 格式</p>
<h3 id="数据集对象的预处理"><a href="#数据集对象的预处理" class="headerlink" title="数据集对象的预处理"></a>数据集对象的预处理</h3></li>
<li><p><code>tf.data.Dataset</code> 类中常用的处理方法：</p>
<ul>
<li><code>Dataset.map(f)</code> ：数据集的每个元素应用函数 <code>f</code> ，得到一个新数据集</li>
<li><code>Dataset.shuffle(buffer_size)</code> ：将数据集打乱（设定一个固定大小的缓冲区（Buffer），取出前 <code>buffer_size</code> 个元素放入，从缓冲区中随机采样，采样后的数据输出，对应空缺位置补后续数据，重复采样）</li>
<li><code>Dataset.batch(batch_size)</code> ：将数据集分成批次——每 batch_size 个元素在第 0 维合并，成为一个元素，三维图片张量变为四维张量<blockquote>
<p><code>image = tf.image.rot90(image)</code> 将图片image逆时针旋转90度</p>
</blockquote>
<h3 id="数据集元素的获取与使用"><a href="#数据集元素的获取与使用" class="headerlink" title="数据集元素的获取与使用"></a>数据集元素的获取与使用</h3></li>
</ul>
</li>
<li><p><code>for</code> 循环迭代</p>
</li>
<li><p><code>iter()</code> 显式创建 Python 迭代器，<code>next()</code> 获取下一个元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataset = tf.data.Dataset.from_tensor_slices((A, B, C, ...))</span><br><span class="line">it = <span class="built_in">iter</span>(dataset)</span><br><span class="line">a_0, b_0, c_0, ... = <span class="built_in">next</span>(it)</span><br><span class="line">a_1, b_1, c_1, ... = <span class="built_in">next</span>(it)</span><br></pre></td></tr></table></figure>
<h3 id="3-4-4-TFRecord-：TensorFlow-数据集存储格式"><a href="#3-4-4-TFRecord-：TensorFlow-数据集存储格式" class="headerlink" title="3.4.4. TFRecord ：TensorFlow 数据集存储格式"></a>3.4.4. TFRecord ：TensorFlow 数据集存储格式</h3></li>
<li><p>可视为一组序列化的 <code>tf.train.Example</code> 元素所组成的列表文件，<code>tf.train.Example</code> 又由若干个 <code>tf.train.Feature</code> 的字典组成</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;   <span class="comment"># example 1 (tf.train.Example)</span></span><br><span class="line">      <span class="string">&#x27;feature_1&#x27;</span>: tf.train.Feature,</span><br><span class="line">      ...</span><br><span class="line">      <span class="string">&#x27;feature_k&#x27;</span>: tf.train.Feature</span><br><span class="line">  &#125;,</span><br><span class="line">  ...</span><br><span class="line">  &#123;   <span class="comment"># example N (tf.train.Example)</span></span><br><span class="line">      <span class="string">&#x27;feature_1&#x27;</span>: tf.train.Feature,</span><br><span class="line">      ...</span><br><span class="line">      <span class="string">&#x27;feature_k&#x27;</span>: tf.train.Feature</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li>
<li><p>存储为TFRecord：</p>
<ul>
<li>读入元素到内存</li>
<li>建立 Feature 字典，再建立 <code>tf.train.Feature</code> 的字典，并将元素转换为<code>tf.train.Example</code>对象</li>
<li>序列化为字符串，通过 <code>tf.io.TFRecordWriter</code> 写入文件</li>
</ul>
</li>
<li><p>读取TFRecord：</p>
<ul>
<li><code>tf.data.TFRecordDataset</code> 读入原始文件，获得 <code>tf.data.Dataset</code> 对象</li>
<li><code>Dataset.map(f)</code> 取 <code>tf.io.parse_single_example</code>，反序列化 <code>tf.train.Example</code> 字符串</li>
</ul>
</li>
<li><p>存储：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.io.TFRecordWriter(tfrecord_file) <span class="keyword">as</span> writer: <span class="comment"># tfrecord_file 为保存的目录</span></span><br><span class="line">  <span class="keyword">for</span> filename, label <span class="keyword">in</span> <span class="built_in">zip</span>(train_filenames, train_labels):</span><br><span class="line">      image = <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>).read()     <span class="comment"># 读取数据集图片到内存，image 为一个 Byte 类型的字符串</span></span><br><span class="line">      feature = &#123;                             <span class="comment"># 建立 tf.train.Feature 字典</span></span><br><span class="line">          <span class="string">&#x27;image&#x27;</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),  <span class="comment"># 图片是一个 Bytes 对象</span></span><br><span class="line">          <span class="string">&#x27;label&#x27;</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))   <span class="comment"># 标签是一个 Int 对象</span></span><br><span class="line">      &#125;</span><br><span class="line">      example = tf.train.Example(features=tf.train.Features(feature=feature)) <span class="comment"># 通过字典建立 Example</span></span><br><span class="line">      writer.write(example.SerializeToString())   <span class="comment"># 将Example序列化并写入 TFRecord 文件</span></span><br></pre></td></tr></table></figure></li>
<li><p>读取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">raw_dataset = tf.data.TFRecordDataset(tfrecord_file)    <span class="comment"># 读取 TFRecord 文件</span></span><br><span class="line"></span><br><span class="line">feature_description = &#123; <span class="comment"># 类似于数据集的 “描述文件”，定义Feature结构，告诉解码器每个Feature的类型是什么</span></span><br><span class="line">    <span class="string">&#x27;image&#x27;</span>: tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">    <span class="string">&#x27;label&#x27;</span>: tf.io.FixedLenFeature([], tf.int64),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_example</span>(<span class="params">example_string</span>):</span> <span class="comment"># 将 TFRecord 文件中的每一个序列化的 tf.train.Example 解码</span></span><br><span class="line">    feature_dict = tf.io.parse_single_example(example_string, feature_description)</span><br><span class="line">    feature_dict[<span class="string">&#x27;image&#x27;</span>] = tf.io.decode_jpeg(feature_dict[<span class="string">&#x27;image&#x27;</span>])    <span class="comment"># 解码</span></span><br><span class="line">    <span class="keyword">return</span> feature_dict[<span class="string">&#x27;image&#x27;</span>], feature_dict[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line">dataset = raw_dataset.<span class="built_in">map</span>(_parse_example)</span><br></pre></td></tr></table></figure>
<h1 id="TensorFlow-模型导出"><a href="#TensorFlow-模型导出" class="headerlink" title="TensorFlow 模型导出"></a>TensorFlow 模型导出</h1></li>
<li><p>将训练好的整个模型完整导出为一系列标准格式的文件，以在不同的平台上部署模型</p>
</li>
</ul>
<h2 id="SavedModel-完整导出模型"><a href="#SavedModel-完整导出模型" class="headerlink" title="SavedModel 完整导出模型"></a>SavedModel 完整导出模型</h2><ul>
<li><p>SavedModel 包含了 TensorFlow 程序的完整信息：参数的权值和计算的流程</p>
</li>
<li><p>使用继承 <code>tf.keras.Model</code> 类建立的 Keras 模型，其需要导出到 SavedModel 格式的方法（比如 <code>call</code> ）都需要使用 <code>@tf.function</code> 修饰</p>
</li>
<li><p>导出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.saved_model.save(model, <span class="string">&quot;保存的目标文件夹名称&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = tf.saved_model.load(<span class="string">&quot;保存的目标文件夹名称&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>继承 <code>tf.keras.Model</code> 类建立的 Keras 模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.flatten = tf.keras.layers.Flatten()</span><br><span class="line">        self.dense1 = tf.keras.layers.Dense(units=<span class="number">100</span>, activation=tf.nn.relu)</span><br><span class="line">        self.dense2 = tf.keras.layers.Dense(units=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @tf.function  # 转化为 SavedModel 支持的计算图</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span>         <span class="comment"># [batch_size, 28, 28, 1]</span></span><br><span class="line">        x = self.flatten(inputs)    <span class="comment"># [batch_size, 784]</span></span><br><span class="line">        x = self.dense1(x)          <span class="comment"># [batch_size, 100]</span></span><br><span class="line">        x = self.dense2(x)          <span class="comment"># [batch_size, 10]</span></span><br><span class="line">        output = tf.nn.softmax(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">model = MLP()</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="Keras自有的模型导出格式"><a href="#Keras自有的模型导出格式" class="headerlink" title="Keras自有的模型导出格式"></a>Keras自有的模型导出格式</h2><ul>
<li><p>导出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;mnist_cnn.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.models.load_model(<span class="string">&quot;mnist_cnn.h5&quot;</span>)</span><br></pre></td></tr></table></figure>

</li>
</ul>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章推荐</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\02\03\Hands-on-ML-2\" rel="bookmark">Hands-on Machine Learning（2）</a></div>
        <div class="popular-posts-excerpt"><p><p>《Hands-on Machine Learning》第二部分阅读笔记（2）用 tf 自定义模型训练网络</p></p></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\02\04\Hands-on-ML-3\" rel="bookmark">Hands-on Machine Learning（3）</a></div>
        <div class="popular-posts-excerpt"><p><p>《Hands-on Machine Learning》第二部分阅读笔记（3）数据接口与 CNN</p></p></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\02\06\Hands-on-ML-4\" rel="bookmark">Hands-on Machine Learning（4）</a></div>
        <div class="popular-posts-excerpt"><p><p>《Hands-on Machine Learning》第二部分阅读笔记（4）RNN 和 1D CNN 处理序列</p></p></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\02\06\Hands-on-ML-5\" rel="bookmark">Hands-on Machine Learning（5）</a></div>
        <div class="popular-posts-excerpt"><p><p>《Hands-on Machine Learning》第二部分阅读笔记（5）字符级RNN、单词级RNN、基于RNN的编码-解码器</p></p></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\02\07\Hands-on-ML-6\" rel="bookmark">Hands-on Machine Learning（6）</a></div>
        <div class="popular-posts-excerpt"><p><p>《Hands-on Machine Learning》第二部分阅读笔记（6）RNN 的注意力机制（Transformer 架构）</p></p></div>
    </li>
  </ul>

        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Thomas-Li 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="Thomas-Li 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Thomas-Li
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://thomas-li-sjtu.github.io/2020/11/07/%E7%AE%80%E5%8D%95%E7%B2%97%E6%9A%B4TensorFlow2/" title="简单粗暴TensorFlow2">https://thomas-li-sjtu.github.io/2020/11/07/简单粗暴TensorFlow2/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <div>
      
        
      
      </div>

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i> Python</a>
              <a href="/tags/TensorFlow/" rel="tag"><i class="fa fa-tag"></i> TensorFlow</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/11/05/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="prev" title="现代密码学（1）对称密码">
      <i class="fa fa-chevron-left"></i> 现代密码学（1）对称密码
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/11/07/Python%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_1%E5%9F%BA%E7%A1%80/" rel="next" title="Python深度学习（1）基础">
      Python深度学习（1）基础 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
      <!-- require APlayer -->
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
      <script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
      <!-- require MetingJS-->
      <script src="https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js"></script> 
      <!--������-->   
      <meting-js
        server="netease"
        id="2655164600"
        type="playlist" 
        mini="false"
        fixed="false"
        list-folded="true"
        autoplay="false"
        volume="0.4"
        theme="#FADFA3"
        order="random"
        loop="all"
        preload="auto"
        mutex="true">
      </meting-js>

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
      
      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow-%E5%9F%BA%E7%A1%80"><span class="nav-number">1.</span> <span class="nav-text">TensorFlow 基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#with%E8%AF%AD%E5%8F%A5"><span class="nav-number">1.1.</span> <span class="nav-text">with语句</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.1.1.</span> <span class="nav-text">其他介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F"><span class="nav-number">1.2.</span> <span class="nav-text">张量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-number">1.3.</span> <span class="nav-text">自动求导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.4.</span> <span class="nav-text">示例：线性回归</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow-%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E4%B8%8E%E8%AE%AD%E7%BB%83"><span class="nav-number">2.</span> <span class="nav-text">TensorFlow 模型建立与训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%EF%BC%88Model%EF%BC%89%E4%B8%8E%E5%B1%82%EF%BC%88Layer%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">模型（Model）与层（Layer）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Keras-%E7%9A%84%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="nav-number">2.1.1.</span> <span class="nav-text">Keras 的全连接层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-MLP"><span class="nav-number">2.2.</span> <span class="nav-text">示例：多层感知机 MLP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%8F%8A%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9A-tf-keras-datasets"><span class="nav-number">2.2.1.</span> <span class="nav-text">数据获取及预处理： tf.keras.datasets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%84%E5%BB%BA%EF%BC%9A-tf-keras-Model-%E5%92%8C-tf-keras-layers"><span class="nav-number">2.2.2.</span> <span class="nav-text">模型的构建： tf.keras.Model 和 tf.keras.layers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%EF%BC%9A-tf-keras-losses-%E5%92%8C-tf-keras-optimizer"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3. 模型的训练： tf.keras.losses 和 tf.keras.optimizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%EF%BC%9A-tf-keras-metrics"><span class="nav-number">2.2.4.</span> <span class="nav-text">模型的评估： tf.keras.metrics</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN"><span class="nav-number">2.3.</span> <span class="nav-text">卷积神经网络 CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="nav-number">2.3.1.</span> <span class="nav-text">预备知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-Keras-%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2. Keras 实现卷积神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-Keras-%E4%B8%AD%E9%A2%84%E5%AE%9A%E4%B9%89%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.3.3.</span> <span class="nav-text">2.3.3. Keras 中预定义的卷积神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-RNN"><span class="nav-number">2.4.</span> <span class="nav-text">循环神经网络 RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86-1"><span class="nav-number">2.4.1.</span> <span class="nav-text">预备知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-%E7%A4%BA%E4%BE%8B%EF%BC%9ARNN-%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E5%B0%BC%E9%87%87%E9%A3%8E%E6%A0%BC%E6%96%87%E6%9C%AC"><span class="nav-number">2.4.2.</span> <span class="nav-text">2.4.2. 示例：RNN 自动生成尼采风格文本</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow-%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97"><span class="nav-number">3.</span> <span class="nav-text">TensorFlow 常用模块</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%EF%BC%9APython%E8%BF%AD%E4%BB%A3%E5%99%A8"><span class="nav-number">3.1.</span> <span class="nav-text">预备知识：Python迭代器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-train-Checkpoint%E7%B1%BB-%EF%BC%9A%E5%8F%98%E9%87%8F%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E6%81%A2%E5%A4%8D"><span class="nav-number">3.2.</span> <span class="nav-text">tf.train.Checkpoint类 ：变量的保存与恢复</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98"><span class="nav-number">3.2.1.</span> <span class="nav-text">保存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-%E8%BD%BD%E5%85%A5"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2. 载入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%A0%E9%99%A4%E6%97%A7%E7%9A%84-Checkpoint"><span class="nav-number">3.2.3.</span> <span class="nav-text">删除旧的 Checkpoint</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorBoard%EF%BC%9A%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">3.3.</span> <span class="nav-text">TensorBoard：训练过程可视化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-data-%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%9E%84%E5%BB%BA%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.4.</span> <span class="nav-text">tf.data ：数据集的构建与预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%BB%BA%E7%AB%8B"><span class="nav-number">3.4.1.</span> <span class="nav-text">数据集对象的建立</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.4.2.</span> <span class="nav-text">数据集对象的预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%85%83%E7%B4%A0%E7%9A%84%E8%8E%B7%E5%8F%96%E4%B8%8E%E4%BD%BF%E7%94%A8"><span class="nav-number">3.4.3.</span> <span class="nav-text">数据集元素的获取与使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-4-TFRecord-%EF%BC%9ATensorFlow-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F"><span class="nav-number">3.4.4.</span> <span class="nav-text">3.4.4. TFRecord ：TensorFlow 数据集存储格式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow-%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%87%BA"><span class="nav-number">4.</span> <span class="nav-text">TensorFlow 模型导出</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SavedModel-%E5%AE%8C%E6%95%B4%E5%AF%BC%E5%87%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.1.</span> <span class="nav-text">SavedModel 完整导出模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keras%E8%87%AA%E6%9C%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%87%BA%E6%A0%BC%E5%BC%8F"><span class="nav-number">4.2.</span> <span class="nav-text">Keras自有的模型导出格式</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Thomas-Li"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Thomas-Li</p>
  <div class="site-description" itemprop="description">Stay hungry. Stay foolish.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">189</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">37</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/thomas-li-sjtu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thomas-li-sjtu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/thomasli2017" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;thomasli2017" rel="noopener" target="_blank"><i class="fa fa-csdn fa-fw"></i>CSDN</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://rooki3ray.github.io/" title="https:&#x2F;&#x2F;rooki3ray.github.io&#x2F;" rel="noopener" target="_blank">rooki3ray</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://entropy2333.github.io/" title="https:&#x2F;&#x2F;entropy2333.github.io&#x2F;" rel="noopener" target="_blank">entropy2333</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://schenk75.github.io/" title="https:&#x2F;&#x2F;schenk75.github.io&#x2F;" rel="noopener" target="_blank">Schenk75</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://ainevsia.github.io/" title="https:&#x2F;&#x2F;ainevsia.github.io&#x2F;" rel="noopener" target="_blank">Ainevsia</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Thomas-Li</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.8m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">27:09</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
<div class="moon-menu">
  <div class="moon-menu-items">
    
    <div class="moon-menu-item" onclick="back2bottom()">
      <i class='fa fa-chevron-down'></i>    </div>
    
    <div class="moon-menu-item" onclick="back2top()">
      <i class='fa fa-chevron-up'></i>    </div>
    
  </div>
  <div class="moon-menu-button" onclick="moonMenuClick()">
    <svg class="moon-menu-svg">
      <circle class="moon-menu-cricle" cx="50%" cy="50%" r="44%"></circle>
      <circle class="moon-menu-border" cx="50%" cy="50%" r="48%"></circle>
      <g class="moon-menu-points">
        <circle class="moon-menu-point" r=".2rem" cx="0" cy="-.8rem"></circle>
        <circle class="moon-menu-point" r=".2rem"></circle>
        <circle class="moon-menu-point" r=".2rem" cx="0" cy=".8rem"></circle>
      </g>
    </svg>
    <div class="moon-menu-icon">
    </div>
    <div class="moon-menu-text">
    </div>
  </div>
</div>
<script src="/js/injector.js"></script>

  
    <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  
</body>
</html>
